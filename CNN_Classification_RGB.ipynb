{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, train accuracy 0.24\n",
      "step 200, train accuracy 0.6\n",
      "step 400, train accuracy 0.65\n",
      "step 600, train accuracy 0.65\n",
      "step 800, train accuracy 0.68\n",
      "step 1000, train accuracy 0.61\n",
      "step 1200, train accuracy 0.68\n",
      "step 1400, train accuracy 0.63\n",
      "step 1600, train accuracy 0.74\n",
      "step 1800, train accuracy 0.74\n",
      "step 2000, train accuracy 0.61\n",
      "step 2200, train accuracy 0.68\n",
      "step 2400, train accuracy 0.64\n",
      "step 2600, train accuracy 0.68\n",
      "step 2800, train accuracy 0.59\n",
      "step 3000, train accuracy 0.74\n",
      "step 3200, train accuracy 0.75\n",
      "step 3400, train accuracy 0.78\n",
      "step 3600, train accuracy 0.64\n",
      "step 3800, train accuracy 0.84\n",
      "step 4000, train accuracy 0.68\n",
      "step 4200, train accuracy 0.62\n",
      "step 4400, train accuracy 0.74\n",
      "step 4600, train accuracy 0.75\n",
      "step 4800, train accuracy 0.68\n",
      "step 5000, train accuracy 0.72\n",
      "step 5200, train accuracy 0.66\n",
      "step 5400, train accuracy 0.72\n",
      "step 5600, train accuracy 0.76\n",
      "step 5800, train accuracy 0.66\n",
      "step 6000, train accuracy 0.75\n",
      "step 6200, train accuracy 0.71\n",
      "step 6400, train accuracy 0.8\n",
      "step 6600, train accuracy 0.74\n",
      "step 6800, train accuracy 0.68\n",
      "step 7000, train accuracy 0.67\n",
      "step 7200, train accuracy 0.74\n",
      "step 7400, train accuracy 0.77\n",
      "step 7600, train accuracy 0.7\n",
      "step 7800, train accuracy 0.69\n",
      "step 8000, train accuracy 0.8\n",
      "step 8200, train accuracy 0.75\n",
      "step 8400, train accuracy 0.72\n",
      "step 8600, train accuracy 0.77\n",
      "step 8800, train accuracy 0.83\n",
      "step 9000, train accuracy 0.81\n",
      "step 9200, train accuracy 0.66\n",
      "step 9400, train accuracy 0.71\n",
      "step 9600, train accuracy 0.74\n",
      "step 9800, train accuracy 0.69\n",
      "step 10000, train accuracy 0.82\n",
      "step 10200, train accuracy 0.73\n",
      "step 10400, train accuracy 0.75\n",
      "step 10600, train accuracy 0.75\n",
      "step 10800, train accuracy 0.75\n",
      "step 11000, train accuracy 0.77\n",
      "step 11200, train accuracy 0.82\n",
      "step 11400, train accuracy 0.69\n",
      "step 11600, train accuracy 0.8\n",
      "step 11800, train accuracy 0.81\n",
      "step 12000, train accuracy 0.82\n",
      "step 12200, train accuracy 0.79\n",
      "step 12400, train accuracy 0.75\n",
      "step 12600, train accuracy 0.8\n",
      "step 12800, train accuracy 0.74\n",
      "step 13000, train accuracy 0.79\n",
      "step 13200, train accuracy 0.74\n",
      "step 13400, train accuracy 0.76\n",
      "step 13600, train accuracy 0.79\n",
      "step 13800, train accuracy 0.77\n",
      "step 14000, train accuracy 0.79\n",
      "step 14200, train accuracy 0.75\n",
      "step 14400, train accuracy 0.74\n",
      "step 14600, train accuracy 0.82\n",
      "step 14800, train accuracy 0.76\n",
      "step 15000, train accuracy 0.84\n",
      "step 15200, train accuracy 0.76\n",
      "step 15400, train accuracy 0.78\n",
      "step 15600, train accuracy 0.8\n",
      "step 15800, train accuracy 0.84\n",
      "step 16000, train accuracy 0.77\n",
      "step 16200, train accuracy 0.83\n",
      "step 16400, train accuracy 0.72\n",
      "step 16600, train accuracy 0.75\n",
      "step 16800, train accuracy 0.75\n",
      "step 17000, train accuracy 0.78\n",
      "step 17200, train accuracy 0.85\n",
      "step 17400, train accuracy 0.89\n",
      "step 17600, train accuracy 0.84\n",
      "step 17800, train accuracy 0.84\n",
      "step 18000, train accuracy 0.8\n",
      "step 18200, train accuracy 0.82\n",
      "step 18400, train accuracy 0.83\n",
      "step 18600, train accuracy 0.82\n",
      "step 18800, train accuracy 0.76\n",
      "step 19000, train accuracy 0.81\n",
      "step 19200, train accuracy 0.79\n",
      "step 19400, train accuracy 0.81\n",
      "step 19600, train accuracy 0.83\n",
      "step 19800, train accuracy 0.83\n",
      "step 20000, train accuracy 0.76\n",
      "step 20200, train accuracy 0.78\n",
      "step 20400, train accuracy 0.84\n",
      "step 20600, train accuracy 0.83\n",
      "step 20800, train accuracy 0.8\n",
      "step 21000, train accuracy 0.9\n",
      "step 21200, train accuracy 0.79\n",
      "step 21400, train accuracy 0.87\n",
      "step 21600, train accuracy 0.9\n",
      "step 21800, train accuracy 0.85\n",
      "step 22000, train accuracy 0.78\n",
      "step 22200, train accuracy 0.85\n",
      "step 22400, train accuracy 0.82\n",
      "step 22600, train accuracy 0.84\n",
      "step 22800, train accuracy 0.86\n",
      "step 23000, train accuracy 0.88\n",
      "step 23200, train accuracy 0.8\n",
      "step 23400, train accuracy 0.88\n",
      "step 23600, train accuracy 0.79\n",
      "step 23800, train accuracy 0.93\n",
      "step 24000, train accuracy 0.8\n",
      "step 24200, train accuracy 0.89\n",
      "step 24400, train accuracy 0.83\n",
      "step 24600, train accuracy 0.91\n",
      "step 24800, train accuracy 0.94\n",
      "step 25000, train accuracy 0.83\n",
      "step 25200, train accuracy 0.84\n",
      "step 25400, train accuracy 0.78\n",
      "step 25600, train accuracy 0.9\n",
      "step 25800, train accuracy 0.9\n",
      "step 26000, train accuracy 0.83\n",
      "step 26200, train accuracy 0.9\n",
      "step 26400, train accuracy 0.82\n",
      "step 26600, train accuracy 0.88\n",
      "step 26800, train accuracy 0.9\n",
      "step 27000, train accuracy 0.87\n",
      "step 27200, train accuracy 0.97\n",
      "step 27400, train accuracy 0.88\n",
      "step 27600, train accuracy 0.9\n",
      "step 27800, train accuracy 0.89\n",
      "step 28000, train accuracy 0.93\n",
      "step 28200, train accuracy 0.91\n",
      "step 28400, train accuracy 0.9\n",
      "step 28600, train accuracy 0.91\n",
      "step 28800, train accuracy 0.89\n",
      "step 29000, train accuracy 0.89\n",
      "step 29200, train accuracy 0.93\n",
      "step 29400, train accuracy 0.94\n",
      "step 29600, train accuracy 0.9\n",
      "step 29800, train accuracy 0.87\n",
      "step 30000, train accuracy 0.91\n",
      "step 30200, train accuracy 0.88\n",
      "step 30400, train accuracy 0.91\n",
      "step 30600, train accuracy 0.92\n",
      "step 30800, train accuracy 0.88\n",
      "step 31000, train accuracy 0.89\n",
      "step 31200, train accuracy 0.89\n",
      "step 31400, train accuracy 0.95\n",
      "step 31600, train accuracy 0.93\n",
      "step 31800, train accuracy 0.91\n",
      "step 32000, train accuracy 0.93\n",
      "step 32200, train accuracy 0.92\n",
      "step 32400, train accuracy 0.92\n",
      "step 32600, train accuracy 0.91\n",
      "step 32800, train accuracy 0.91\n",
      "step 33000, train accuracy 0.93\n",
      "step 33200, train accuracy 0.9\n",
      "step 33400, train accuracy 0.93\n",
      "step 33600, train accuracy 0.94\n",
      "step 33800, train accuracy 0.93\n",
      "step 34000, train accuracy 0.95\n",
      "step 34200, train accuracy 0.92\n",
      "step 34400, train accuracy 0.96\n",
      "step 34600, train accuracy 0.86\n",
      "step 34800, train accuracy 0.95\n",
      "step 35000, train accuracy 0.95\n",
      "step 35200, train accuracy 0.92\n",
      "step 35400, train accuracy 0.93\n",
      "step 35600, train accuracy 0.94\n",
      "step 35800, train accuracy 0.95\n",
      "step 36000, train accuracy 0.93\n",
      "step 36200, train accuracy 0.9\n",
      "step 36400, train accuracy 0.97\n",
      "step 36600, train accuracy 0.92\n",
      "step 36800, train accuracy 0.95\n",
      "step 37000, train accuracy 0.95\n",
      "step 37200, train accuracy 0.95\n",
      "step 37400, train accuracy 0.96\n",
      "step 37600, train accuracy 0.91\n",
      "step 37800, train accuracy 0.89\n",
      "step 38000, train accuracy 0.93\n",
      "step 38200, train accuracy 0.96\n",
      "step 38400, train accuracy 0.95\n",
      "step 38600, train accuracy 0.98\n",
      "step 38800, train accuracy 0.96\n",
      "step 39000, train accuracy 0.95\n",
      "step 39200, train accuracy 0.96\n",
      "step 39400, train accuracy 0.9\n",
      "step 39600, train accuracy 0.95\n",
      "step 39800, train accuracy 0.94\n",
      "step 40000, train accuracy 0.99\n",
      "step 40200, train accuracy 0.96\n",
      "step 40400, train accuracy 0.97\n",
      "step 40600, train accuracy 0.97\n",
      "step 40800, train accuracy 0.97\n",
      "step 41000, train accuracy 0.95\n",
      "step 41200, train accuracy 0.95\n",
      "step 41400, train accuracy 0.95\n",
      "step 41600, train accuracy 0.98\n",
      "step 41800, train accuracy 0.99\n",
      "step 42000, train accuracy 0.96\n",
      "step 42200, train accuracy 0.99\n",
      "step 42400, train accuracy 0.96\n",
      "step 42600, train accuracy 0.94\n",
      "step 42800, train accuracy 0.95\n",
      "step 43000, train accuracy 0.96\n",
      "step 43200, train accuracy 0.94\n",
      "step 43400, train accuracy 0.97\n",
      "step 43600, train accuracy 0.96\n",
      "step 43800, train accuracy 0.95\n",
      "step 44000, train accuracy 0.95\n",
      "step 44200, train accuracy 0.99\n",
      "step 44400, train accuracy 0.99\n",
      "step 44600, train accuracy 0.98\n",
      "step 44800, train accuracy 1\n",
      "step 45000, train accuracy 0.99\n",
      "step 45200, train accuracy 0.97\n",
      "step 45400, train accuracy 0.98\n",
      "step 45600, train accuracy 0.99\n",
      "step 45800, train accuracy 0.97\n",
      "step 46000, train accuracy 1\n",
      "step 46200, train accuracy 0.97\n",
      "step 46400, train accuracy 0.99\n",
      "step 46600, train accuracy 0.95\n",
      "step 46800, train accuracy 0.96\n",
      "step 47000, train accuracy 0.93\n",
      "step 47200, train accuracy 0.97\n",
      "step 47400, train accuracy 0.98\n",
      "step 47600, train accuracy 1\n",
      "step 47800, train accuracy 1\n",
      "step 48000, train accuracy 0.97\n",
      "step 48200, train accuracy 0.99\n",
      "step 48400, train accuracy 0.96\n",
      "step 48600, train accuracy 0.99\n",
      "step 48800, train accuracy 1\n",
      "step 49000, train accuracy 0.94\n",
      "step 49200, train accuracy 0.97\n",
      "step 49400, train accuracy 0.98\n",
      "step 49600, train accuracy 0.98\n",
      "step 49800, train accuracy 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50000, train accuracy 0.97\n",
      "step 50200, train accuracy 0.97\n",
      "step 50400, train accuracy 0.98\n",
      "step 50600, train accuracy 0.99\n",
      "step 50800, train accuracy 0.99\n",
      "step 51000, train accuracy 0.99\n",
      "step 51200, train accuracy 0.99\n",
      "step 51400, train accuracy 0.98\n",
      "step 51600, train accuracy 0.95\n",
      "step 51800, train accuracy 0.97\n",
      "step 52000, train accuracy 0.99\n",
      "step 52200, train accuracy 0.98\n",
      "step 52400, train accuracy 0.98\n",
      "step 52600, train accuracy 0.99\n",
      "step 52800, train accuracy 0.99\n",
      "step 53000, train accuracy 1\n",
      "step 53200, train accuracy 1\n",
      "step 53400, train accuracy 0.99\n",
      "step 53600, train accuracy 1\n",
      "step 53800, train accuracy 0.98\n",
      "step 54000, train accuracy 1\n",
      "step 54200, train accuracy 0.99\n",
      "step 54400, train accuracy 0.99\n",
      "step 54600, train accuracy 0.97\n",
      "step 54800, train accuracy 0.99\n",
      "step 55000, train accuracy 0.99\n",
      "step 55200, train accuracy 0.98\n",
      "step 55400, train accuracy 1\n",
      "step 55600, train accuracy 0.99\n",
      "step 55800, train accuracy 0.99\n",
      "step 56000, train accuracy 1\n",
      "step 56200, train accuracy 0.99\n",
      "step 56400, train accuracy 0.99\n",
      "step 56600, train accuracy 0.98\n",
      "step 56800, train accuracy 0.96\n",
      "step 57000, train accuracy 1\n",
      "step 57200, train accuracy 1\n",
      "step 57400, train accuracy 0.95\n",
      "step 57600, train accuracy 0.99\n",
      "step 57800, train accuracy 0.99\n",
      "step 58000, train accuracy 0.99\n",
      "step 58200, train accuracy 0.98\n",
      "step 58400, train accuracy 1\n",
      "step 58600, train accuracy 1\n",
      "step 58800, train accuracy 0.99\n",
      "step 59000, train accuracy 0.98\n",
      "step 59200, train accuracy 1\n",
      "step 59400, train accuracy 0.99\n",
      "step 59600, train accuracy 0.99\n",
      "step 59800, train accuracy 1\n",
      "step 60000, train accuracy 1\n",
      "step 60200, train accuracy 1\n",
      "step 60400, train accuracy 1\n",
      "step 60600, train accuracy 1\n",
      "step 60800, train accuracy 1\n",
      "step 61000, train accuracy 0.99\n",
      "step 61200, train accuracy 0.99\n",
      "step 61400, train accuracy 1\n",
      "step 61600, train accuracy 1\n",
      "step 61800, train accuracy 1\n",
      "step 62000, train accuracy 1\n",
      "step 62200, train accuracy 1\n",
      "step 62400, train accuracy 1\n",
      "step 62600, train accuracy 1\n",
      "step 62800, train accuracy 0.99\n",
      "step 63000, train accuracy 0.99\n",
      "step 63200, train accuracy 1\n",
      "step 63400, train accuracy 0.99\n",
      "step 63600, train accuracy 0.98\n",
      "step 63800, train accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# start tensorflow interactiveSession                           F1 = 0.7903857746\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 7500])\n",
    "y_ = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 50, 50, 3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([13*13*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 13*13*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "image_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train_RGB/'\n",
    "data_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = pd.get_dummies(data,columns = ['Class'])\n",
    "\n",
    "\n",
    "def get_batch(data,batch_rate,seed):\n",
    "    \"\"\"\n",
    "    本函数实现从训练集中随机抽取部分图像，格式适合输入tensorflow，同时得到样本label的独热编码\n",
    "    input:\n",
    "        batch_rate:    batch占训练集数据的比例\n",
    "        data:          DataFrame       train.csv\n",
    "        seed:          随机种子\n",
    "    output:\n",
    "        train_x:        训练图像tensor\n",
    "        train_y:        训练图像one_hot编码的label\n",
    "    \"\"\"\n",
    "    sample = data.sample(frac = batch_rate, random_state = seed)\n",
    "    sample.index = list(range(0,sample.shape[0]))\n",
    "    one_hot_sample = sample.drop(['ID'], axis = 1)\n",
    "    train_y = one_hot_sample.values\n",
    "    train_x = []\n",
    "    for i in range(0,sample.shape[0]):\n",
    "        image = image_path + sample.ID[i]\n",
    "        im = np.array(Image.open(image))/255.0\n",
    "        im = im.flatten()\n",
    "        train_x.append(im)\n",
    "    train_x = np.array(train_x)\n",
    "    \n",
    "    return train_x,train_y\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_rate = 0.005\n",
    "for i in range(64000):\n",
    "    batch_x,batch_y = get_batch(data,batch_rate,i)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'E:/Age Detection of Indian Actors/test_Bh8pGW3/Test_RGB/'\n",
    "\n",
    "test = pd.read_csv('E:/Age Detection of Indian Actors/test_Bh8pGW3/test.csv')\n",
    "\n",
    "test_x = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    image = image_path + test.ID[i]\n",
    "    im = np.array(Image.open(image))/255.0\n",
    "    im = im.flatten()\n",
    "    test_x.append(im)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.zeros((test.shape[0],3))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test_x[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test_x[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test_x[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test_x[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "test['pred'] = P\n",
    "\n",
    "Dict = {0:'MIDDLE',1:'OLD', 2:'YOUNG'}\n",
    "Class = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    Class.append(Dict.get(test['pred'][i]))\n",
    "    \n",
    "pred = pd.DataFrame({'Class':Class,'ID':test.ID})\n",
    "pred.head()\n",
    "\n",
    "pred.to_csv('50_RGB_pix_pred_64k_batch_new.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, train accuracy 0.55\n",
      "step 200, train accuracy 0.71\n",
      "step 400, train accuracy 0.74\n",
      "step 600, train accuracy 0.75\n",
      "step 800, train accuracy 0.84\n",
      "step 1000, train accuracy 0.72\n",
      "step 1200, train accuracy 0.76\n",
      "step 1400, train accuracy 0.79\n",
      "step 1600, train accuracy 0.88\n",
      "step 1800, train accuracy 0.81\n",
      "step 2000, train accuracy 0.78\n",
      "step 2200, train accuracy 0.87\n",
      "step 2400, train accuracy 0.86\n",
      "step 2600, train accuracy 0.91\n",
      "step 2800, train accuracy 0.87\n",
      "step 3000, train accuracy 0.97\n",
      "step 3200, train accuracy 0.94\n",
      "step 3400, train accuracy 0.95\n",
      "step 3600, train accuracy 0.93\n",
      "step 3800, train accuracy 0.96\n",
      "step 4000, train accuracy 0.94\n",
      "step 4200, train accuracy 0.95\n",
      "step 4400, train accuracy 0.96\n",
      "step 4600, train accuracy 0.98\n",
      "step 4800, train accuracy 0.94\n",
      "step 5000, train accuracy 0.99\n",
      "step 5200, train accuracy 0.98\n",
      "step 5400, train accuracy 0.97\n",
      "step 5600, train accuracy 0.98\n",
      "step 5800, train accuracy 1\n",
      "step 6000, train accuracy 0.99\n",
      "step 6200, train accuracy 0.98\n",
      "step 6400, train accuracy 0.99\n",
      "step 6600, train accuracy 0.99\n",
      "step 6800, train accuracy 1\n",
      "step 7000, train accuracy 1\n",
      "step 7200, train accuracy 1\n",
      "step 7400, train accuracy 0.99\n",
      "step 7600, train accuracy 1\n",
      "step 7800, train accuracy 1\n",
      "step 8000, train accuracy 1\n",
      "step 8200, train accuracy 1\n",
      "step 8400, train accuracy 1\n",
      "step 8600, train accuracy 1\n",
      "step 8800, train accuracy 1\n",
      "step 9000, train accuracy 1\n",
      "step 9200, train accuracy 1\n",
      "step 9400, train accuracy 0.99\n",
      "step 9600, train accuracy 1\n",
      "step 9800, train accuracy 1\n"
     ]
    }
   ],
   "source": [
    "#使用Adam优化器更新参数                                          F1 = 0.8126883665\n",
    "# start tensorflow interactiveSession                           \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 7500])\n",
    "y_ = tf.placeholder(\"float\", [None, 3])\n",
    "\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1 = weight_variable([5, 5, 3, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 50, 50, 3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 13*13*64])\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([13*13*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "image_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train_RGB/'\n",
    "data_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = pd.get_dummies(data,columns = ['Class'])\n",
    "\n",
    "\n",
    "def get_batch(data,batch_rate,seed):\n",
    "    \"\"\"\n",
    "    本函数实现从训练集中随机抽取部分图像，格式适合输入tensorflow，同时得到样本label的独热编码\n",
    "    input:\n",
    "        batch_rate:    batch占训练集数据的比例\n",
    "        data:          DataFrame       train.csv\n",
    "        seed:          随机种子\n",
    "    output:\n",
    "        train_x:        训练图像tensor\n",
    "        train_y:        训练图像one_hot编码的label\n",
    "    \"\"\"\n",
    "    sample = data.sample(frac = batch_rate, random_state = seed)\n",
    "    sample.index = list(range(0,sample.shape[0]))\n",
    "    one_hot_sample = sample.drop(['ID'], axis = 1)\n",
    "    train_y = one_hot_sample.values\n",
    "    train_x = []\n",
    "    for i in range(0,sample.shape[0]):\n",
    "        image = image_path + sample.ID[i]\n",
    "        im = np.array(Image.open(image))/255.0\n",
    "        im = im.flatten()\n",
    "        train_x.append(im)\n",
    "    train_x = np.array(train_x)\n",
    "    \n",
    "    return train_x,train_y\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_rate = 0.005\n",
    "for i in range(10000):\n",
    "    batch_x,batch_y = get_batch(data,batch_rate,i)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'E:/Age Detection of Indian Actors/test_Bh8pGW3/Test_RGB/'\n",
    "\n",
    "test = pd.read_csv('E:/Age Detection of Indian Actors/test_Bh8pGW3/test.csv')\n",
    "\n",
    "test_x = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    image = image_path + test.ID[i]\n",
    "    im = np.array(Image.open(image))/255.0\n",
    "    im = im.flatten()\n",
    "    test_x.append(im)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.zeros((test.shape[0],3))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test_x[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test_x[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test_x[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test_x[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "test['pred'] = P\n",
    "\n",
    "Dict = {0:'MIDDLE',1:'OLD', 2:'YOUNG'}\n",
    "Class = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    Class.append(Dict.get(test['pred'][i]))\n",
    "    \n",
    "pred = pd.DataFrame({'Class':Class,'ID':test.ID})\n",
    "pred.head()\n",
    "\n",
    "pred.to_csv('50_RGB_pix_pred_10k_batch_adam.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 用2个3 * 3卷积层替代一个5 * 5卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "step 0, train accuracy 0.34\n",
      "step 200, train accuracy 0.68\n",
      "step 400, train accuracy 0.73\n",
      "step 600, train accuracy 0.76\n",
      "step 800, train accuracy 0.76\n",
      "step 1000, train accuracy 0.7\n",
      "step 1200, train accuracy 0.77\n",
      "step 1400, train accuracy 0.74\n",
      "step 1600, train accuracy 0.85\n",
      "step 1800, train accuracy 0.81\n",
      "step 2000, train accuracy 0.75\n",
      "step 2200, train accuracy 0.85\n",
      "step 2400, train accuracy 0.82\n",
      "step 2600, train accuracy 0.86\n",
      "step 2800, train accuracy 0.79\n",
      "step 3000, train accuracy 0.92\n",
      "step 3200, train accuracy 0.94\n",
      "step 3400, train accuracy 0.9\n",
      "step 3600, train accuracy 0.91\n",
      "step 3800, train accuracy 0.92\n",
      "step 4000, train accuracy 0.96\n",
      "step 4200, train accuracy 0.91\n",
      "step 4400, train accuracy 0.93\n",
      "step 4600, train accuracy 0.97\n",
      "step 4800, train accuracy 0.93\n",
      "step 5000, train accuracy 0.98\n",
      "step 5200, train accuracy 0.96\n",
      "step 5400, train accuracy 0.96\n",
      "step 5600, train accuracy 0.99\n",
      "step 5800, train accuracy 0.99\n",
      "step 6000, train accuracy 0.97\n",
      "step 6200, train accuracy 0.98\n",
      "step 6400, train accuracy 0.99\n",
      "step 6600, train accuracy 0.97\n",
      "step 6800, train accuracy 1\n",
      "step 7000, train accuracy 0.99\n",
      "step 7200, train accuracy 1\n",
      "step 7400, train accuracy 1\n",
      "step 7600, train accuracy 1\n",
      "step 7800, train accuracy 1\n",
      "step 8000, train accuracy 1\n",
      "step 8200, train accuracy 1\n",
      "step 8400, train accuracy 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1ba33d2ca3d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mbatch_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.005\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-1ba33d2ca3d8>\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(data, batch_rate, seed)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2579\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2580\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2581\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用Adam优化器更新参数                                         F1 = 0.8169077758 \n",
    "# start tensorflow interactiveSession                           \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 7500])\n",
    "y_ = tf.placeholder(\"float\", [None, 3])\n",
    "x_image = tf.reshape(x, [-1, 50, 50, 3])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1_1 = weight_variable([3, 3, 3, 32])\n",
    "b_conv1_1 = bias_variable([32])\n",
    "\n",
    "w_conv1_2 = weight_variable([3, 3, 32, 32])\n",
    "b_conv1_2 = bias_variable([32])\n",
    "\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x_image, w_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, w_conv1_2) + b_conv1_2)\n",
    "h_pool1 = max_pool_2x2(h_conv1_2)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2_1 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2_1 = bias_variable([64])\n",
    "\n",
    "w_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_2 = bias_variable([64])\n",
    "\n",
    "h_conv2_1 = tf.nn.relu(conv2d(h_pool1, w_conv2_1) + b_conv2_1)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, w_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 13*13*64])\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([13*13*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "image_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train_RGB/'\n",
    "data_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = pd.get_dummies(data,columns = ['Class'])\n",
    "\n",
    "\n",
    "def get_batch(data,batch_rate,seed):\n",
    "    \"\"\"\n",
    "    本函数实现从训练集中随机抽取部分图像，格式适合输入tensorflow，同时得到样本label的独热编码\n",
    "    input:\n",
    "        batch_rate:    batch占训练集数据的比例\n",
    "        data:          DataFrame       train.csv\n",
    "        seed:          随机种子\n",
    "    output:\n",
    "        train_x:        训练图像tensor\n",
    "        train_y:        训练图像one_hot编码的label\n",
    "    \"\"\"\n",
    "    sample = data.sample(frac = batch_rate, random_state = seed)\n",
    "    sample.index = list(range(0,sample.shape[0]))\n",
    "    one_hot_sample = sample.drop(['ID'], axis = 1)\n",
    "    train_y = one_hot_sample.values\n",
    "    train_x = []\n",
    "    for i in range(0,sample.shape[0]):\n",
    "        image = image_path + sample.ID[i]\n",
    "        im = np.array(Image.open(image))/255.0\n",
    "        im = im.flatten()\n",
    "        train_x.append(im)\n",
    "    train_x = np.array(train_x)\n",
    "    \n",
    "    return train_x,train_y\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_rate = 0.005\n",
    "for i in range(10000):\n",
    "    batch_x,batch_y = get_batch(data,batch_rate,i)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'E:/Age Detection of Indian Actors/test_Bh8pGW3/Test_RGB/'\n",
    "\n",
    "test = pd.read_csv('E:/Age Detection of Indian Actors/test_Bh8pGW3/test.csv')\n",
    "\n",
    "test_x = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    image = image_path + test.ID[i]\n",
    "    im = np.array(Image.open(image))/255.0\n",
    "    im = im.flatten()\n",
    "    test_x.append(im)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.zeros((test.shape[0],3))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test_x[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test_x[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test_x[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test_x[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "test['pred'] = P\n",
    "\n",
    "Dict = {0:'MIDDLE',1:'OLD', 2:'YOUNG'}\n",
    "Class = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    Class.append(Dict.get(test['pred'][i]))\n",
    "    \n",
    "pred = pd.DataFrame({'Class':Class,'ID':test.ID})\n",
    "pred.head()\n",
    "\n",
    "pred.to_csv('50_RGB_pix_pred_10k_batch_adam_3_3conv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 增加两层卷积层，增大batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.103015\n",
      "step 200, train accuracy 0.683417\n",
      "step 400, train accuracy 0.728643\n",
      "step 600, train accuracy 0.763819\n",
      "step 800, train accuracy 0.801508\n",
      "step 1000, train accuracy 0.809045\n",
      "step 1200, train accuracy 0.821608\n",
      "step 1400, train accuracy 0.871859\n",
      "step 1600, train accuracy 0.909548\n",
      "step 1800, train accuracy 0.91206\n",
      "step 2000, train accuracy 0.922111\n",
      "step 2200, train accuracy 0.937186\n",
      "step 2400, train accuracy 0.964824\n",
      "step 2600, train accuracy 0.9799\n",
      "step 2800, train accuracy 0.9799\n",
      "step 3000, train accuracy 0.987437\n",
      "step 3200, train accuracy 0.987437\n",
      "step 3400, train accuracy 0.997487\n",
      "step 3600, train accuracy 0.997487\n",
      "step 3800, train accuracy 0.997487\n",
      "step 4000, train accuracy 0.997487\n",
      "step 4200, train accuracy 1\n",
      "step 4400, train accuracy 1\n",
      "step 4600, train accuracy 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2ea3b4c6daad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[0mbatch_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.02\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-2ea3b4c6daad>\u001b[0m in \u001b[0;36mget_batch\u001b[1;34m(data, batch_rate, seed)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mtrain_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0mtrain_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用Adam优化器更新参数                                         F1 = 0.8196202532\n",
    "# start tensorflow interactiveSession                           \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 7500])\n",
    "y_ = tf.placeholder(\"float\", [None, 3])\n",
    "x_image = tf.reshape(x, [-1, 50, 50, 3])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1_1 = weight_variable([3, 3, 3, 32])\n",
    "b_conv1_1 = bias_variable([32])\n",
    "\n",
    "w_conv1_2 = weight_variable([3, 3, 32, 32])\n",
    "b_conv1_2 = bias_variable([32])\n",
    "\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x_image, w_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, w_conv1_2) + b_conv1_2)\n",
    "h_pool1 = max_pool_2x2(h_conv1_2)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2_1 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2_1 = bias_variable([64])\n",
    "\n",
    "w_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_2 = bias_variable([64])\n",
    "\n",
    "h_conv2_1 = tf.nn.relu(conv2d(h_pool1, w_conv2_1) + b_conv2_1)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, w_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "# third conv layer\n",
    "w_conv3_1 = weight_variable([3, 3, 64, 128])\n",
    "b_conv3_1 = bias_variable([128])\n",
    "\n",
    "w_conv3_2 = weight_variable([3, 3, 128, 128])\n",
    "b_conv3_2 = bias_variable([128])\n",
    "\n",
    "h_conv3_1 = tf.nn.relu(conv2d(h_pool2, w_conv3_1) + b_conv3_1)\n",
    "h_conv3_2 = tf.nn.relu(conv2d(h_conv3_1, w_conv3_2) + b_conv3_2)\n",
    "h_pool3 = max_pool_2x2(h_conv3_2)\n",
    "\n",
    "\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 7*7*128])\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([7*7*128, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "image_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train_RGB/'\n",
    "data_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = pd.get_dummies(data,columns = ['Class'])\n",
    "\n",
    "\n",
    "def get_batch(data,batch_rate,seed):\n",
    "    \"\"\"\n",
    "    本函数实现从训练集中随机抽取部分图像，格式适合输入tensorflow，同时得到样本label的独热编码\n",
    "    input:\n",
    "        batch_rate:    batch占训练集数据的比例\n",
    "        data:          DataFrame       train.csv\n",
    "        seed:          随机种子\n",
    "    output:\n",
    "        train_x:        训练图像tensor\n",
    "        train_y:        训练图像one_hot编码的label\n",
    "    \"\"\"\n",
    "    sample = data.sample(frac = batch_rate, random_state = seed)\n",
    "    sample.index = list(range(0,sample.shape[0]))\n",
    "    one_hot_sample = sample.drop(['ID'], axis = 1)\n",
    "    train_y = one_hot_sample.values\n",
    "    train_x = []\n",
    "    for i in range(0,sample.shape[0]):\n",
    "        image = image_path + sample.ID[i]\n",
    "        im = np.array(Image.open(image))/255.0\n",
    "        im = im.flatten()\n",
    "        train_x.append(im)\n",
    "    train_x = np.array(train_x)\n",
    "    \n",
    "    return train_x,train_y\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_rate = 0.02\n",
    "for i in range(10000):\n",
    "    batch_x,batch_y = get_batch(data,batch_rate,i)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'E:/Age Detection of Indian Actors/test_Bh8pGW3/Test_RGB/'\n",
    "\n",
    "test = pd.read_csv('E:/Age Detection of Indian Actors/test_Bh8pGW3/test.csv')\n",
    "\n",
    "test_x = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    image = image_path + test.ID[i]\n",
    "    im = np.array(Image.open(image))/255.0\n",
    "    im = im.flatten()\n",
    "    test_x.append(im)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.zeros((test.shape[0],3))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test_x[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test_x[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test_x[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test_x[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "test['pred'] = P\n",
    "\n",
    "Dict = {0:'MIDDLE',1:'OLD', 2:'YOUNG'}\n",
    "Class = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    Class.append(Dict.get(test['pred'][i]))\n",
    "    \n",
    "pred = pd.DataFrame({'Class':Class,'ID':test.ID})\n",
    "pred.head()\n",
    "\n",
    "pred.to_csv('50_RGB_pix_pred_10k_batch_adam_3_3_3conv.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 保持4+2CNN结构，增大batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1714: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, train accuracy 0.236181\n",
      "step 200, train accuracy 0.673367\n",
      "step 400, train accuracy 0.703518\n",
      "step 600, train accuracy 0.78392\n",
      "step 800, train accuracy 0.80402\n",
      "step 1000, train accuracy 0.809045\n",
      "step 1200, train accuracy 0.81407\n",
      "step 1400, train accuracy 0.864322\n",
      "step 1600, train accuracy 0.894472\n",
      "step 1800, train accuracy 0.909548\n",
      "step 2000, train accuracy 0.929648\n",
      "step 2200, train accuracy 0.934673\n",
      "step 2400, train accuracy 0.944724\n",
      "step 2600, train accuracy 0.969849\n",
      "step 2800, train accuracy 0.984925\n",
      "step 3000, train accuracy 0.98995\n",
      "step 3200, train accuracy 0.987437\n",
      "step 3400, train accuracy 0.992462\n",
      "step 3600, train accuracy 0.997487\n",
      "step 3800, train accuracy 0.98995\n",
      "step 4000, train accuracy 0.994975\n",
      "step 4200, train accuracy 1\n",
      "step 4400, train accuracy 1\n",
      "step 4600, train accuracy 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b9863cb2f1b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"step %d, train accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   2397\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2398\u001b[0m     \"\"\"\n\u001b[1;32m-> 2399\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2401\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gradient\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5246\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5247\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 5248\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#使用Adam优化器更新参数                                         F1 = 0.81285\n",
    "# start tensorflow interactiveSession                           \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#####################################################     Net Define     ##################################################### \n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "# pooling\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Create the model\n",
    "# placeholder\n",
    "x = tf.placeholder(\"float\", [None, 7500])\n",
    "y_ = tf.placeholder(\"float\", [None, 3])\n",
    "x_image = tf.reshape(x, [-1, 50, 50, 3])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "# first convolutinal layer\n",
    "w_conv1_1 = weight_variable([3, 3, 3, 32])\n",
    "b_conv1_1 = bias_variable([32])\n",
    "\n",
    "w_conv1_2 = weight_variable([3, 3, 32, 32])\n",
    "b_conv1_2 = bias_variable([32])\n",
    "\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x_image, w_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1, w_conv1_2) + b_conv1_2)\n",
    "h_pool1 = max_pool_2x2(h_conv1_2)\n",
    "\n",
    "# second convolutional layer\n",
    "w_conv2_1 = weight_variable([3, 3, 32, 64])\n",
    "b_conv2_1 = bias_variable([64])\n",
    "\n",
    "w_conv2_2 = weight_variable([3, 3, 64, 64])\n",
    "b_conv2_2 = bias_variable([64])\n",
    "\n",
    "h_conv2_1 = tf.nn.relu(conv2d(h_pool1, w_conv2_1) + b_conv2_1)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1, w_conv2_2) + b_conv2_2)\n",
    "h_pool2 = max_pool_2x2(h_conv2_2)\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 13*13*64])\n",
    "# densely connected layer\n",
    "w_fc1 = weight_variable([13*13*64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n",
    "\n",
    "# dropout\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# readout layer\n",
    "w_fc2 = weight_variable([1024, 3])\n",
    "b_fc2 = bias_variable([3])\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, w_fc2) + b_fc2)\n",
    "\n",
    "# train and evaluate the model\n",
    "#交叉熵作为损失函数\n",
    "delta = 1e-7\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv+delta))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "#####################################################       Train     ##################################################### \n",
    "\n",
    "image_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train_RGB/'\n",
    "data_path = 'E:/Age Detection of Indian Actors/train_DETg9GD/Train.csv'\n",
    "\n",
    "data = pd.read_csv(data_path)\n",
    "data = pd.get_dummies(data,columns = ['Class'])\n",
    "\n",
    "\n",
    "def get_batch(data,batch_rate,seed):\n",
    "    \"\"\"\n",
    "    本函数实现从训练集中随机抽取部分图像，格式适合输入tensorflow，同时得到样本label的独热编码\n",
    "    input:\n",
    "        batch_rate:    batch占训练集数据的比例\n",
    "        data:          DataFrame       train.csv\n",
    "        seed:          随机种子\n",
    "    output:\n",
    "        train_x:        训练图像tensor\n",
    "        train_y:        训练图像one_hot编码的label\n",
    "    \"\"\"\n",
    "    sample = data.sample(frac = batch_rate, random_state = seed)\n",
    "    sample.index = list(range(0,sample.shape[0]))\n",
    "    one_hot_sample = sample.drop(['ID'], axis = 1)\n",
    "    train_y = one_hot_sample.values\n",
    "    train_x = []\n",
    "    for i in range(0,sample.shape[0]):\n",
    "        image = image_path + sample.ID[i]\n",
    "        im = np.array(Image.open(image))/255.0\n",
    "        im = im.flatten()\n",
    "        train_x.append(im)\n",
    "    train_x = np.array(train_x)\n",
    "    \n",
    "    return train_x,train_y\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "batch_rate = 0.02\n",
    "for i in range(5000):\n",
    "    batch_x,batch_y = get_batch(data,batch_rate,i)\n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:batch_x, y_:batch_y, keep_prob:1.0})\n",
    "        print (\"step %d, train accuracy %g\" %(i, train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch_x, y_:batch_y, keep_prob:0.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = 'E:/Age Detection of Indian Actors/test_Bh8pGW3/Test_RGB/'\n",
    "\n",
    "test = pd.read_csv('E:/Age Detection of Indian Actors/test_Bh8pGW3/test.csv')\n",
    "\n",
    "test_x = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    image = image_path + test.ID[i]\n",
    "    im = np.array(Image.open(image))/255.0\n",
    "    im = im.flatten()\n",
    "    test_x.append(im)\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.zeros((test.shape[0],3))\n",
    "\n",
    "pred = tf.argmax(y_conv, 1)\n",
    "\n",
    "test_x_0 = test_x[0:1500]\n",
    "test_y_0 = test_y[0:1500]\n",
    "P_0 = pred.eval(feed_dict={x:test_x_0, y_:test_y_0, keep_prob:1.0})\n",
    "\n",
    "test_x_1 = test_x[1500:3000]\n",
    "test_y_1 = test_y[1500:3000]\n",
    "P_1 = pred.eval(feed_dict={x:test_x_1, y_:test_y_1, keep_prob:1.0})\n",
    "\n",
    "test_x_2 = test_x[3000:4500]\n",
    "test_y_2 = test_y[3000:4500]\n",
    "P_2 = pred.eval(feed_dict={x:test_x_2, y_:test_y_2, keep_prob:1.0})\n",
    "\n",
    "test_x_3 = test_x[4500:]\n",
    "test_y_3 = test_y[4500:]\n",
    "P_3 = pred.eval(feed_dict={x:test_x_3, y_:test_y_3, keep_prob:1.0})\n",
    "\n",
    "P = np.hstack([P_0,P_1,P_2,P_3])\n",
    "\n",
    "test['pred'] = P\n",
    "\n",
    "Dict = {0:'MIDDLE',1:'OLD', 2:'YOUNG'}\n",
    "Class = []\n",
    "for i in range(0,test.shape[0]):\n",
    "    Class.append(Dict.get(test['pred'][i]))\n",
    "    \n",
    "pred = pd.DataFrame({'Class':Class,'ID':test.ID})\n",
    "pred.head()\n",
    "\n",
    "pred.to_csv('50_RGB_pix_pred_10k_batch_adam_3_3_2conv.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
